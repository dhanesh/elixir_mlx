defmodule Mlx.PythonComparisonTest do
  @moduledoc """
  Compares Elixir MLX results against pre-generated Python MLX reference values.

  Reference values are stored in test/fixtures/python_refs/references.json,
  generated by test/scripts/generate_references.py using Python MLX.

  To regenerate fixtures after an MLX version upgrade:
      uv run --with mlx python3 test/scripts/generate_references.py
  """
  use ExUnit.Case, async: false

  @refs Jason.decode!(File.read!("test/fixtures/python_refs/references.json"))

  # Default tolerance — f32 precision differences between Metal and reference
  @default_atol 1.0e-5

  setup do
    Nx.default_backend(Mlx.Backend)
    :ok
  end

  defp assert_close(actual, expected, atol \\ @default_atol) when is_list(expected) do
    actual_list = Nx.to_flat_list(actual)

    Enum.zip(actual_list, List.flatten(expected))
    |> Enum.each(fn {a, e} ->
      assert_in_delta a, e, atol, "expected #{a} to be within #{atol} of #{e}"
    end)
  end

  defp assert_close_scalar(actual, expected, atol \\ @default_atol) do
    assert_in_delta Nx.to_number(actual),
                    expected,
                    atol,
                    "expected #{Nx.to_number(actual)} to be within #{atol} of #{expected}"
  end

  defp to_tensor(data) when is_list(data) do
    Nx.tensor(data)
  end

  # ── Unary ops ─────────────────────────────────────────

  for op <- ~w(exp log sqrt rsqrt abs negative sin cos tanh erf sigmoid) do
    @op op
    test "#{op} matches Python MLX" do
      ref = @refs[@op]
      input = to_tensor(ref["input"])
      expected = ref["output"]

      result =
        case @op do
          "exp" -> Nx.exp(input)
          "log" -> Nx.log(input)
          "sqrt" -> Nx.sqrt(input)
          "rsqrt" -> Nx.rsqrt(input)
          "abs" -> Nx.abs(input)
          "negative" -> Nx.negate(input)
          "sin" -> Nx.sin(input)
          "cos" -> Nx.cos(input)
          "tanh" -> Nx.tanh(input)
          "erf" -> Nx.erf(input)
          "sigmoid" -> Nx.sigmoid(input)
        end

      assert_close(result, expected)
    end
  end

  test "square matches Python MLX" do
    ref = @refs["square"]
    input = to_tensor(ref["input"])
    result = Nx.pow(input, 2)
    assert_close(result, ref["output"])
  end

  # ── Binary ops ────────────────────────────────────────

  for op <- ~w(add multiply subtract divide power) do
    @op op
    test "#{op} matches Python MLX" do
      ref = @refs[@op]
      left = to_tensor(ref["left"])
      right = to_tensor(ref["right"])
      expected = ref["output"]

      result =
        case @op do
          "add" -> Nx.add(left, right)
          "multiply" -> Nx.multiply(left, right)
          "subtract" -> Nx.subtract(left, right)
          "divide" -> Nx.divide(left, right)
          "power" -> Nx.pow(left, right)
        end

      assert_close(result, expected)
    end
  end

  # ── Reduction ops ─────────────────────────────────────

  test "sum_all matches Python MLX" do
    ref = @refs["sum_all"]
    input = to_tensor(ref["input"])
    assert_close_scalar(Nx.sum(input), ref["output"])
  end

  test "sum_axis0 matches Python MLX" do
    ref = @refs["sum_axis0"]
    input = to_tensor(ref["input"])
    assert_close(Nx.sum(input, axes: [0]), ref["output"])
  end

  test "sum_axis1 matches Python MLX" do
    ref = @refs["sum_axis1"]
    input = to_tensor(ref["input"])
    assert_close(Nx.sum(input, axes: [1]), ref["output"])
  end

  test "mean_all matches Python MLX" do
    ref = @refs["mean_all"]
    input = to_tensor(ref["input"])
    assert_close_scalar(Nx.mean(input), ref["output"])
  end

  test "max_all matches Python MLX" do
    ref = @refs["max_all"]
    input = to_tensor(ref["input"])
    assert_close_scalar(Nx.reduce_max(input), ref["output"])
  end

  test "min_all matches Python MLX" do
    ref = @refs["min_all"]
    input = to_tensor(ref["input"])
    assert_close_scalar(Nx.reduce_min(input), ref["output"])
  end

  test "argmax_axis1 matches Python MLX" do
    ref = @refs["argmax_axis1"]
    input = to_tensor(ref["input"])
    result = Nx.argmax(input, axis: 1)
    expected_ints = ref["output"]
    actual_ints = Nx.to_flat_list(result)

    Enum.zip(actual_ints, expected_ints)
    |> Enum.each(fn {a, e} -> assert a == e end)
  end

  # ── Softmax ───────────────────────────────────────────

  test "softmax matches Python MLX" do
    ref = @refs["softmax"]
    input = to_tensor(ref["input"])

    result =
      Nx.exp(input) |> then(fn x -> Nx.divide(x, Nx.sum(x, axes: [-1], keep_axes: true)) end)

    assert_close(result, ref["output"])
  end

  # ── Activation functions (Mlx.NN vs Python mlx.nn) ───

  for op <- ~w(gelu silu relu relu6 elu softplus celu log_sigmoid mish leaky_relu selu) do
    @op op
    test "nn.#{op} matches Python mlx.nn.#{op}" do
      ref = @refs[@op]
      input = to_tensor(ref["input"])
      expected = ref["output"]

      result =
        case @op do
          "gelu" -> Mlx.NN.gelu(input)
          "silu" -> Mlx.NN.silu(input)
          "relu" -> Mlx.NN.relu(input)
          "relu6" -> Mlx.NN.relu6(input)
          "elu" -> Mlx.NN.elu(input)
          "softplus" -> Mlx.NN.softplus(input)
          "celu" -> Mlx.NN.celu(input)
          "log_sigmoid" -> Mlx.NN.log_sigmoid(input)
          "mish" -> Mlx.NN.mish(input)
          "leaky_relu" -> Mlx.NN.leaky_relu(input)
          "selu" -> Mlx.NN.selu(input)
        end

      assert_close(result, expected, 1.0e-4)
    end
  end

  test "nn.hard_swish matches Python mlx.nn.hardswish" do
    ref = @refs["hard_swish"]
    input = to_tensor(ref["input"])
    result = Mlx.NN.hard_swish(input)
    assert_close(result, ref["output"], 1.0e-4)
  end

  # ── Matmul ────────────────────────────────────────────

  test "matmul matches Python MLX" do
    ref = @refs["matmul"]
    left = to_tensor(ref["left"])
    right = to_tensor(ref["right"])
    result = Nx.dot(left, right)
    assert_close(result, ref["output"])
  end

  # ── Linalg ────────────────────────────────────────────

  test "eigvalsh matches Python MLX" do
    ref = @refs["eigvalsh"]
    input = to_tensor(ref["input"])
    {eigenvalues, _eigenvectors} = Nx.LinAlg.eigh(input)
    assert_close(eigenvalues, ref["output"], 1.0e-4)
  end

  test "svd singular values match Python MLX" do
    ref = @refs["svd_s"]
    input = to_tensor(ref["input"])
    {_u, s, _vt} = Nx.LinAlg.svd(input)
    assert_close(s, ref["output"], 1.0e-4)
  end

  test "inv matches Python MLX" do
    ref = @refs["inv"]
    input = to_tensor(ref["input"])
    result = Mlx.Linalg.inv(input)
    assert_close(result, ref["output"], 1.0e-4)
  end

  test "norm matches Python MLX" do
    ref = @refs["norm"]
    input = to_tensor(ref["input"])
    result = Nx.LinAlg.norm(input)
    assert_close_scalar(result, ref["output"], 1.0e-4)
  end

  # ── Cumulative ops ────────────────────────────────────

  test "cumsum matches Python MLX" do
    ref = @refs["cumsum"]
    input = to_tensor(ref["input"])
    result = Nx.cumulative_sum(input, axis: 0)
    assert_close(result, ref["output"])
  end

  test "cumprod matches Python MLX" do
    ref = @refs["cumprod"]
    input = to_tensor(ref["input"])
    result = Nx.cumulative_product(input, axis: 0)
    assert_close(result, ref["output"])
  end
end
